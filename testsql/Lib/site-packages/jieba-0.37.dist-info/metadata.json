{"license": "MIT", "name": "jieba", "metadata_version": "2.0", "generator": "bdist_wheel (0.24.0)", "summary": "Chinese Words Segementation Utilities", "version": "0.37", "extensions": {"python.details": {"project_urls": {"Home": "https://github.com/fxsjy/jieba"}, "document_names": {"description": "DESCRIPTION.rst"}, "contacts": [{"role": "author", "email": "ccnusjy@gmail.com", "name": "Sun, Junyi"}]}}, "keywords": ["NLP", "tokenizing", "Chinese", "word", "segementation"], "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Natural Language :: Chinese (Simplified)", "Natural Language :: Chinese (Traditional)", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Topic :: Text Processing", "Topic :: Text Processing :: Indexing", "Topic :: Text Processing :: Linguistic"]}